{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26431994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliffatur/coding/kalbe-cdt-intern/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d949f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database ...\n",
      "Tables found: ['mkt_bps_angka_kelahiran', 'mkt_bps_gini_ratio', 'mkt_bps_inflasi_nasional', 'mkt_bps_jumlah_balita', 'mkt_bps_jumlah_ibu_hamil', 'mkt_bps_jumlah_penduduk', 'mkt_bps_jumlah_penduduk_per_usia', 'mkt_bps_jumlah_pns', 'mkt_bps_jumlah_tenaga_kesehatan', 'mkt_bps_pengeluaran_per_kapita', 'mkt_bps_persentase_bayi_asi_eksklusif', 'mkt_bps_produk_domestik_reg_bruto', 'mkt_bps_umr', 'ref_mkt_seki_devisa', 'ref_mkt_seki_exchange', 'ref_mkt_seki_export_import', 'ref_mkt_seki_ihk', 'ref_mkt_seki_indeks_harga', 'ref_mkt_seki_indonesia_ringkasan', 'ref_mkt_seki_inflasi', 'ref_mkt_seki_interest', 'ref_mkt_seki_investasi', 'ref_mkt_seki_pdb', 'ref_mkt_seki_savings', 'ref_mkt_seki_transaksi_berjalan_internasional']\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"bps_seki.db\"\n",
    "\n",
    "def load_all_tables(db_path: str) -> Dict[str, pd.DataFrame]:\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    insp = inspect(engine)\n",
    "    tables = {}\n",
    "    for name in insp.get_table_names():\n",
    "        df = pd.read_sql_table(name, engine)\n",
    "        tables[name] = df\n",
    "    return tables\n",
    "\n",
    "print(\"Loading database ...\")\n",
    "tables = load_all_tables(DB_PATH)\n",
    "print(f\"Tables found: {list(tables.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7063faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents created: 113934\n"
     ]
    }
   ],
   "source": [
    "def df_to_docs(name: str, df: pd.DataFrame) -> List[Document]:\n",
    "    docs = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = \"\\n\".join([f\"{col}: {row[col]}\" for col in df.columns])\n",
    "        docs.append(Document(page_content=text, metadata={\"table\": name, \"row\": i}))\n",
    "    return docs\n",
    "\n",
    "all_docs = []\n",
    "for name, df in tables.items():\n",
    "    all_docs.extend(df_to_docs(name, df))\n",
    "\n",
    "print(f\"Total documents created: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99aff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore built successfully!\n",
      "RetrievalQA pipeline (new version) ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from operator import itemgetter\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "OLLAMA_MODEL = \"gemma3:latest\"\n",
    "\n",
    "#embedding & ingest to vectorstore database\n",
    "emb = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vectorstore = FAISS.from_documents(all_docs, emb)\n",
    "print(\"Vectorstore built successfully!\")\n",
    "\n",
    "#ollama llm\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3949a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a knowledgeable assistant. Use the following context to answer the question.\n",
    "If the answer cannot be found in the context, say \"I'm not sure based on the provided information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "# PERBAIKAN: Extract question string untuk retriever\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_docs(retriever.invoke(x[\"question\"])),  # ‚Üê Perbaikan\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71e10f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot ready! Type your question below (type 'exit' to quit).\n",
      "\n",
      "\n",
      "====================\n",
      "Question: halo\n",
      "\n",
      "Answer:\n",
      " I'm not sure based on the provided information.\n",
      "====================\n",
      "\n",
      "\n",
      "====================\n",
      "Question: bisa jelaskan siapa kamu?\n",
      "\n",
      "Answer:\n",
      " I am a knowledgeable assistant designed to answer questions based on the provided context.\n",
      "====================\n",
      "\n",
      "\n",
      "Session ended.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# --- State definition ---\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    continue_chat: bool\n",
    "\n",
    "# --- Define graph ---\n",
    "g = StateGraph(QAState)\n",
    "\n",
    "# --- Node function ---\n",
    "def ask(state: QAState):\n",
    "    user_input = state[\"question\"]\n",
    "    \n",
    "    # Jalankan chain dengan input\n",
    "    result = qa_chain.invoke({\"question\": user_input})\n",
    "    \n",
    "    # Ambil hasil jawaban\n",
    "    if isinstance(result, dict):\n",
    "        answer = result.get(\"answer\") or result.get(\"output_text\") or str(result)\n",
    "    else:\n",
    "        answer = str(result)\n",
    "    \n",
    "    print(\"\\n====================\")\n",
    "    print(\"Question:\", user_input)\n",
    "    print(\"\\nAnswer:\\n\", answer)\n",
    "    print(\"====================\\n\")\n",
    "    \n",
    "    return {\"answer\": answer, \"continue_chat\": False}\n",
    "\n",
    "# Conditional edge function\n",
    "def should_continue(state: QAState):\n",
    "    return \"ask\" if state.get(\"continue_chat\", False) else END\n",
    "\n",
    "# Tambahkan node dan transisi\n",
    "g.add_node(\"ask\", ask)\n",
    "g.set_entry_point(\"ask\")\n",
    "g.add_conditional_edges(\"ask\", should_continue)\n",
    "\n",
    "# Kompilasi graph\n",
    "app = g.compile()\n",
    "\n",
    "print(\"Chatbot ready! Type your question below (type 'exit' to quit).\\n\")\n",
    "\n",
    "# Loop manual untuk interaksi\n",
    "while True:\n",
    "    try:\n",
    "        user_question = input(\"Your question> \")\n",
    "        \n",
    "        if user_question.lower() in ['exit', 'quit', 'q']:\n",
    "            print(\"\\nSession ended.\")\n",
    "            break\n",
    "        \n",
    "        # Invoke graph dengan question\n",
    "        result = app.invoke({\"question\": user_question, \"answer\": \"\", \"continue_chat\": False})\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSession ended.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac96d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9c0e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database ...\n",
      "Tables found: ['mkt_bps_angka_kelahiran', 'mkt_bps_gini_ratio', 'mkt_bps_inflasi_nasional', 'mkt_bps_jumlah_balita', 'mkt_bps_jumlah_ibu_hamil', 'mkt_bps_jumlah_penduduk', 'mkt_bps_jumlah_penduduk_per_usia', 'mkt_bps_jumlah_pns', 'mkt_bps_jumlah_tenaga_kesehatan', 'mkt_bps_pengeluaran_per_kapita', 'mkt_bps_persentase_bayi_asi_eksklusif', 'mkt_bps_produk_domestik_reg_bruto', 'mkt_bps_umr', 'ref_mkt_seki_devisa', 'ref_mkt_seki_exchange', 'ref_mkt_seki_export_import', 'ref_mkt_seki_ihk', 'ref_mkt_seki_indeks_harga', 'ref_mkt_seki_indonesia_ringkasan', 'ref_mkt_seki_inflasi', 'ref_mkt_seki_interest', 'ref_mkt_seki_investasi', 'ref_mkt_seki_pdb', 'ref_mkt_seki_savings', 'ref_mkt_seki_transaksi_berjalan_internasional']\n",
      "\n",
      "Creating documents...\n",
      "Total documents created:\n",
      "  - Metadata docs: 25\n",
      "  - Summary docs: 2658\n",
      "  - Row docs: 56964\n",
      "  - TOTAL: 59647\n",
      "\n",
      "Building vectorstore...\n",
      "Vectorstore built successfully!\n",
      "Enhanced QA chain ready!\n",
      "ü§ñ BPS Chatbot Ready!\n",
      "==================================================\n",
      "Commands:\n",
      "  - Type your question in Indonesian or English\n",
      "  - Type 'exit', 'quit', or 'q' to quit\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "‚ùì Question: halo\n",
      "\n",
      "üí° Answer:\n",
      "Maaf, saya tidak mengerti apa yang Anda maksud. Bisakah Anda mengulangi pertanyaan Anda dalam bahasa Indonesia atau bahasa Inggris?\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "‚ùì Question: bisa jelaskan latar belakangmu?\n",
      "\n",
      "üí° Answer:\n",
      "Maaf, saya adalah asisten yang berspesialisasi dalam data BPS Indonesia. Saya memiliki akses ke data dari tabel `mkt_bps_jumlah_balita` dan `mkt_bps_persentase_bayi_asi_eksklusif`. Saya dapat memberikan informasi tentang jumlah balita dan persentase bayi ASI eksklusif berdasarkan data yang tersedia. Untuk saat ini, saya tidak memiliki informasi tentang latar belakang saya sendiri.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "‚ùì Question: berikan analisis mengenai jumlah balita di tahun 2020\n",
      "\n",
      "üí° Answer:\n",
      "Maaf, saya tidak memiliki data yang cukup untuk memberikan analisis mengenai jumlah balita pada tahun 2020. Data yang tersedia hanya mencakup tahun 2021. Tabel `mkt_bps_jumlah_penduduk_per_usia` hanya memiliki data untuk tahun 2021.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "‚ùì Question: berikan analisis jumlah balita tahun 2021\n",
      "\n",
      "üí° Answer:\n",
      "Maaf, saya tidak memiliki data yang cukup untuk memberikan analisis jumlah balita pada tahun 2021 berdasarkan data yang tersedia. Data yang saya miliki hanya mencakup jumlah penduduk berdasarkan kelompok usia (agegroup) seperti (20-24), (45-49), (65-69), dan lainnya. Tidak ada informasi mengenai jumlah balita (biasanya di bawah 5 tahun) dalam dataset ini.  Informasi ini berasal dari tabel `mkt_bps_jumlah_penduduk_per_usia`.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "‚ùì Question: baik berikan informasi mengenai data mkt_bps_jumlah balita\n",
      "\n",
      "üí° Answer:\n",
      "Maaf, saya tidak memiliki data yang cukup untuk menjawab pertanyaan ini secara akurat. Informasi yang tersedia hanya mencakup angka kelahiran (angka kematian bayi) dan tidak menyertakan data mengenai jumlah balita.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "‚ùì Question: jelaskan tabel `mkt_bps_persentase_bayi_asi_eksklusif` isinya apaan\n",
      "\n",
      "üí° Answer:\n",
      "Maaf, saya tidak memiliki cukup data untuk menjawab pertanyaan Anda secara akurat. Tabel `mkt_bps_persentase_bayi_asi_eksklusif` tidak ada dalam dataset yang saya miliki. Saya hanya memiliki data dari tabel `mkt_bps_jumlah_pns` dan `mkt_bps_jumlah_balita`.\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "‚ùì Question: `mkt_bps_jumlah_pns` jelaskan ini\n",
      "\n",
      "üí° Answer:\n",
      "Tabel `mkt_bps_jumlah_pns` berisi jumlah Pegawai Negeri Sipil (PNS) di berbagai wilayah di Indonesia. Berdasarkan data yang tersedia, rata-rata jumlah PNS di Kalimantan Tengah pada tahun 2019 adalah 73121.11. Informasi ini diambil dari ringkasan statistik untuk wilayah `area=KALIMANTAN TENGAH, leveldata=1_PROVINSI`.\n",
      "==================================================\n",
      "\n",
      "\n",
      "üëã Session ended. Thank you!\n",
      "‚úÖ Script completed!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "\n",
    "# %%\n",
    "DB_PATH = \"bps_seki.db\"\n",
    "\n",
    "def load_all_tables(db_path: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load semua tabel dari database SQLite\"\"\"\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    insp = inspect(engine)\n",
    "    tables = {}\n",
    "    for name in insp.get_table_names():\n",
    "        df = pd.read_sql_table(name, engine)\n",
    "        tables[name] = df\n",
    "    return tables\n",
    "\n",
    "print(\"Loading database ...\")\n",
    "tables = load_all_tables(DB_PATH)\n",
    "print(f\"Tables found: {list(tables.keys())}\")\n",
    "\n",
    "# %%\n",
    "# IMPROVEMENT 1: Buat dokumen metadata untuk setiap tabel\n",
    "def create_table_metadata_docs(tables: Dict[str, pd.DataFrame]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Buat dokumen yang menjelaskan struktur dan isi setiap tabel.\n",
    "    Ini membantu LLM memahami schema database.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for table_name, df in tables.items():\n",
    "        # Informasi dasar tabel\n",
    "        metadata = {\n",
    "            \"type\": \"table_metadata\",\n",
    "            \"table\": table_name,\n",
    "            \"row_count\": len(df)\n",
    "        }\n",
    "        \n",
    "        # Deskripsi tabel\n",
    "        columns_info = []\n",
    "        for col in df.columns:\n",
    "            dtype = str(df[col].dtype)\n",
    "            unique_count = df[col].nunique()\n",
    "            \n",
    "            # Ambil sample values (untuk kategorikal)\n",
    "            if unique_count < 50:\n",
    "                sample_values = df[col].unique()[:10].tolist()\n",
    "                columns_info.append(\n",
    "                    f\"- {col} ({dtype}): {unique_count} unique values. \"\n",
    "                    f\"Examples: {sample_values}\"\n",
    "                )\n",
    "            else:\n",
    "                columns_info.append(\n",
    "                    f\"- {col} ({dtype}): {unique_count} unique values\"\n",
    "                )\n",
    "        \n",
    "        content = f\"\"\"\n",
    "Table: {table_name}\n",
    "Description: BPS statistical data table\n",
    "Row count: {len(df)}\n",
    "Columns:\n",
    "{chr(10).join(columns_info)}\n",
    "\n",
    "This table contains BPS (Badan Pusat Statistik) data about {table_name.replace('_', ' ')}.\n",
    "\"\"\"\n",
    "        \n",
    "        docs.append(Document(page_content=content, metadata=metadata))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# IMPROVEMENT 2: Buat dokumen summary untuk setiap tabel\n",
    "def create_table_summary_docs(tables: Dict[str, pd.DataFrame]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Buat summary agregat untuk setiap tabel.\n",
    "    Lebih efisien daripada per-row indexing.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for table_name, df in tables.items():\n",
    "        # Identifikasi kolom kunci\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "        \n",
    "        # Buat summary berdasarkan kategori\n",
    "        if categorical_cols and len(categorical_cols) <= 3:\n",
    "            # Group by kategori dan buat summary\n",
    "            for group_cols in [categorical_cols[:2]] if len(categorical_cols) >= 2 else [categorical_cols[:1]]:\n",
    "                try:\n",
    "                    grouped = df.groupby(group_cols)\n",
    "                    \n",
    "                    for group_key, group_df in grouped:\n",
    "                        if len(group_df) > 0:\n",
    "                            # Buat deskripsi grup\n",
    "                            if isinstance(group_key, tuple):\n",
    "                                group_desc = \", \".join([f\"{col}={val}\" for col, val in zip(group_cols, group_key)])\n",
    "                            else:\n",
    "                                group_desc = f\"{group_cols[0]}={group_key}\"\n",
    "                            \n",
    "                            # Summary statistik untuk numeric columns\n",
    "                            stats = []\n",
    "                            for num_col in numeric_cols:\n",
    "                                if num_col in group_df.columns:\n",
    "                                    mean_val = group_df[num_col].mean()\n",
    "                                    if pd.notna(mean_val):\n",
    "                                        stats.append(f\"{num_col} avg: {mean_val:.2f}\")\n",
    "                            \n",
    "                            content = f\"\"\"\n",
    "Table: {table_name}\n",
    "Filter: {group_desc}\n",
    "Records: {len(group_df)}\n",
    "Statistics: {', '.join(stats) if stats else 'N/A'}\n",
    "\n",
    "Data available for: {group_desc} in {table_name}\n",
    "\"\"\"\n",
    "                            \n",
    "                            metadata = {\n",
    "                                \"type\": \"table_summary\",\n",
    "                                \"table\": table_name,\n",
    "                                \"filter\": group_desc,\n",
    "                                \"row_count\": len(group_df)\n",
    "                            }\n",
    "                            \n",
    "                            docs.append(Document(page_content=content, metadata=metadata))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not create summary for {table_name}: {e}\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# IMPROVEMENT 3: Tetap buat dokumen per-row tapi lebih compact\n",
    "def create_row_docs(tables: Dict[str, pd.DataFrame], sample_ratio: float = 0.3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Buat dokumen per-row dengan format lebih compact.\n",
    "    Sample hanya sebagian data untuk menghindari context overload.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for table_name, df in tables.items():\n",
    "        # Sample data (opsional, hapus jika ingin semua data)\n",
    "        if len(df) > 100:\n",
    "            sampled_df = df.sample(n=int(len(df) * sample_ratio), random_state=42)\n",
    "        else:\n",
    "            sampled_df = df\n",
    "        \n",
    "        for idx, row in sampled_df.iterrows():\n",
    "            # Format lebih compact: hanya kolom penting\n",
    "            important_fields = []\n",
    "            for col, val in row.items():\n",
    "                if pd.notna(val) and val != '':\n",
    "                    important_fields.append(f\"{col}: {val}\")\n",
    "            \n",
    "            content = f\"Table: {table_name}\\n\" + \" | \".join(important_fields[:10])  # Limit 10 fields\n",
    "            \n",
    "            metadata = {\n",
    "                \"type\": \"row_data\",\n",
    "                \"table\": table_name,\n",
    "                \"row_index\": idx\n",
    "            }\n",
    "            \n",
    "            docs.append(Document(page_content=content, metadata=metadata))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Buat semua dokumen\n",
    "print(\"\\nCreating documents...\")\n",
    "metadata_docs = create_table_metadata_docs(tables)\n",
    "summary_docs = create_table_summary_docs(tables)\n",
    "row_docs = create_row_docs(tables, sample_ratio=0.5)  # 50% sampling\n",
    "\n",
    "all_docs = metadata_docs + summary_docs + row_docs\n",
    "\n",
    "print(f\"Total documents created:\")\n",
    "print(f\"  - Metadata docs: {len(metadata_docs)}\")\n",
    "print(f\"  - Summary docs: {len(summary_docs)}\")\n",
    "print(f\"  - Row docs: {len(row_docs)}\")\n",
    "print(f\"  - TOTAL: {len(all_docs)}\")\n",
    "\n",
    "# %%\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "OLLAMA_MODEL = \"gemma3:latest\"\n",
    "\n",
    "# Embedding & ingest to vectorstore database\n",
    "print(\"\\nBuilding vectorstore...\")\n",
    "emb = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vectorstore = FAISS.from_documents(all_docs, emb)\n",
    "print(\"Vectorstore built successfully!\")\n",
    "\n",
    "# Ollama LLM\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)\n",
    "\n",
    "# IMPROVEMENT 4: Retriever dengan filter metadata\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10}  # Ambil lebih banyak untuk filtering\n",
    ")\n",
    "\n",
    "# %%\n",
    "# IMPROVEMENT 5: Enhanced formatting dengan prioritas metadata\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format dokumen dengan prioritas: metadata > summary > row data\"\"\"\n",
    "    # Pisahkan berdasarkan tipe\n",
    "    metadata_docs = [d for d in docs if d.metadata.get(\"type\") == \"table_metadata\"]\n",
    "    summary_docs = [d for d in docs if d.metadata.get(\"type\") == \"table_summary\"]\n",
    "    row_docs = [d for d in docs if d.metadata.get(\"type\") == \"row_data\"]\n",
    "    \n",
    "    formatted = []\n",
    "    \n",
    "    # Prioritaskan metadata\n",
    "    if metadata_docs:\n",
    "        formatted.append(\"=== TABLE SCHEMAS ===\")\n",
    "        formatted.extend([d.page_content for d in metadata_docs[:2]])\n",
    "    \n",
    "    # Lalu summary\n",
    "    if summary_docs:\n",
    "        formatted.append(\"\\n=== DATA SUMMARIES ===\")\n",
    "        formatted.extend([d.page_content for d in summary_docs[:5]])\n",
    "    \n",
    "    # Terakhir row data\n",
    "    if row_docs:\n",
    "        formatted.append(\"\\n=== SAMPLE DATA ===\")\n",
    "        formatted.extend([d.page_content for d in row_docs[:5]])\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# IMPROVEMENT 6: Enhanced prompt dengan instruksi lebih jelas\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a knowledgeable assistant specialized in Indonesian BPS (Badan Pusat Statistik) data.\n",
    "\n",
    "Use the following context to answer the question accurately. The context includes:\n",
    "1. Table schemas (structure and column information)\n",
    "2. Data summaries (aggregated statistics)\n",
    "3. Sample data (actual records)\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "- If the answer involves numbers, provide specific values from the context\n",
    "- If you need to compare data across years, regions, or categories, use the summaries\n",
    "- If the context doesn't contain enough information, say \"I don't have enough data to answer this accurately\"\n",
    "- Always mention which table(s) the information comes from\n",
    "- Format numbers clearly (use thousands separators when appropriate)\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (in Indonesian):\n",
    "\"\"\")\n",
    "\n",
    "# QA Chain\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(\"Enhanced QA chain ready!\")\n",
    "\n",
    "# %%\n",
    "# IMPROVEMENT 7: Better state management dengan history\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    continue_chat: bool\n",
    "    history: List[str]  # Track conversation history\n",
    "\n",
    "g = StateGraph(QAState)\n",
    "\n",
    "def ask(state: QAState):\n",
    "    user_input = state[\"question\"]\n",
    "    \n",
    "    # Jalankan chain\n",
    "    result = qa_chain.invoke({\"question\": user_input})\n",
    "    \n",
    "    # Extract answer\n",
    "    if isinstance(result, dict):\n",
    "        answer = result.get(\"answer\") or result.get(\"output_text\") or str(result)\n",
    "    else:\n",
    "        answer = str(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚ùì Question:\", user_input)\n",
    "    print(\"\\nüí° Answer:\")\n",
    "    print(answer)\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Update history\n",
    "    history = state.get(\"history\", [])\n",
    "    history.append(f\"Q: {user_input}\\nA: {answer}\")\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"continue_chat\": False,\n",
    "        \"history\": history\n",
    "    }\n",
    "\n",
    "def should_continue(state: QAState):\n",
    "    return \"ask\" if state.get(\"continue_chat\", False) else END\n",
    "\n",
    "g.add_node(\"ask\", ask)\n",
    "g.set_entry_point(\"ask\")\n",
    "g.add_conditional_edges(\"ask\", should_continue)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "print(\"ü§ñ BPS Chatbot Ready!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Commands:\")\n",
    "print(\"  - Type your question in Indonesian or English\")\n",
    "print(\"  - Type 'exit', 'quit', or 'q' to quit\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    try:\n",
    "        user_question = input(\"Your question> \")\n",
    "        \n",
    "        if user_question.lower().strip() in ['exit', 'quit', 'q', '']:\n",
    "            print(\"\\nüëã Session ended. Thank you!\")\n",
    "            break\n",
    "        \n",
    "        result = app.invoke({\n",
    "            \"question\": user_question,\n",
    "            \"answer\": \"\",\n",
    "            \"continue_chat\": False,\n",
    "            \"history\": []\n",
    "        })\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã Session ended. Thank you!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "# %%\n",
    "print(\"‚úÖ Script completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
