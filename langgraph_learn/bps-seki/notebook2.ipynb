{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2c2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc86d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated metadata for: ref_mkt_seki_exchange.csv ‚Üí metadata/ref_mkt_seki_exchange.json\n",
      "Generated metadata for: ref_mkt_seki_interest.csv ‚Üí metadata/ref_mkt_seki_interest.json\n",
      "Generated metadata for: ref_mkt_seki_savings.csv ‚Üí metadata/ref_mkt_seki_savings.json\n",
      "Generated metadata for: ref_mkt_seki_ihk.csv ‚Üí metadata/ref_mkt_seki_ihk.json\n",
      "Generated metadata for: ref_mkt_seki_pareto_terpisah.csv ‚Üí metadata/ref_mkt_seki_pareto_terpisah.json\n",
      "Generated metadata for: ref_mkt_seki_indeks_harga.csv ‚Üí metadata/ref_mkt_seki_indeks_harga.json\n",
      "Generated metadata for: ref_mkt_seki_investasi.csv ‚Üí metadata/ref_mkt_seki_investasi.json\n",
      "Generated metadata for: ref_mkt_seki_transaksi_berjalan_internasional.csv ‚Üí metadata/ref_mkt_seki_transaksi_berjalan_internasional.json\n",
      "Generated metadata for: ref_mkt_seki_indonesia_ringkasan.csv ‚Üí metadata/ref_mkt_seki_indonesia_ringkasan.json\n",
      "Generated metadata for: ref_mkt_seki_devisa.csv ‚Üí metadata/ref_mkt_seki_devisa.json\n",
      "Generated metadata for: ref_mkt_seki_inflasi.csv ‚Üí metadata/ref_mkt_seki_inflasi.json\n",
      "Generated metadata for: ref_mkt_seki_export_import.csv ‚Üí metadata/ref_mkt_seki_export_import.json\n",
      "Generated metadata for: ref_mkt_seki_pdb.csv ‚Üí metadata/ref_mkt_seki_pdb.json\n",
      "\n",
      "Selesai! Semua metadata sudah di-generate dan NaN telah diubah menjadi string 'NaN'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n",
      "/tmp/ipykernel_125569/2528319578.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_clean = df.applymap(clean_nan)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "INPUT_DIR = \"data\"\n",
    "OUTPUT_DIR = \"metadata\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_nan(value):\n",
    "    \"\"\"\n",
    "    Convert null, None, NaT to string 'NaN'.\n",
    "    Keep other values unchanged.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"NaN\"\n",
    "    return value\n",
    "\n",
    "# List CSV files only\n",
    "csv_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".csv\")]\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(INPUT_DIR, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Apply null cleaning\n",
    "    df_clean = df.applymap(clean_nan)\n",
    "\n",
    "    table_name = file.replace(\".csv\", \"\")\n",
    "\n",
    "    metadata = {\n",
    "        \"table\": table_name,\n",
    "        \"description\": \"\",\n",
    "        \"columns\": {col: \"\" for col in df_clean.columns},\n",
    "        \"example_rows\": df_clean.head(3).to_dict(orient=\"records\")\n",
    "    }\n",
    "\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{table_name}.json\")\n",
    "\n",
    "    # Save with UTF-8 + pretty formatting\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Generated metadata for: {file} ‚Üí {output_path}\")\n",
    "\n",
    "print(\"\\nSelesai! Semua metadata sudah di-generate dan null telah diubah menjadi string 'NaN'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af93c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menemukan 13 file CSV. Memulai proses...\n",
      "========================================\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_exchange.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_exchange.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_interest.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_interest.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_savings.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_savings.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_ihk.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_ihk.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_pareto_terpisah.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_pareto_terpisah.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_indeks_harga.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_indeks_harga.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_investasi.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_investasi.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_transaksi_berjalan_internasional.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_transaksi_berjalan_internasional.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_indonesia_ringkasan.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_indonesia_ringkasan.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_devisa.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_devisa.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_inflasi.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_inflasi.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_export_import.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_export_import.json\n",
      "\n",
      "üìÇ Memproses: ref_mkt_seki_pdb.csv...\n",
      "   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\n",
      "   ‚úÖ Disimpan: ./data/metadata-SEKI-v2/ref_mkt_seki_pdb.json\n",
      "\n",
      "========================================\n",
      "üéâ Selesai memproses semua file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ollama\n",
    "import os\n",
    "\n",
    "def process_single_file(file_path, output_folder):\n",
    "    \"\"\"\n",
    "    Fungsi helper untuk memproses satu file CSV menjadi JSON metadata.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    base_name = os.path.splitext(filename)[0] # Hilangkan ekstensi .csv\n",
    "    output_path = os.path.join(output_folder, f\"{base_name}.json\")\n",
    "\n",
    "    print(f\"\\nüìÇ Memproses: {filename}...\")\n",
    "\n",
    "    # 1. BACA CSV DAN BUAT SAMPEL\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Gagal membaca file {filename}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Logika Sampling (Pinggir - Tengah - Pinggir)\n",
    "    total_rows = len(df)\n",
    "    if total_rows >= 4:\n",
    "        indices = [0, int(total_rows / 3), int(total_rows * 2 / 3), total_rows - 1]\n",
    "    else:\n",
    "        indices = list(range(total_rows))\n",
    "    \n",
    "    sample_df = df.iloc[indices]\n",
    "    sample_rows = sample_df.where(pd.notnull(sample_df), None).to_dict(orient='records')\n",
    "\n",
    "    # Persiapan Konteks Prompt\n",
    "    columns = df.columns.tolist()\n",
    "    context_row = sample_rows[0] if sample_rows else {}\n",
    "\n",
    "    # 2. GENERATE DESKRIPSI DENGAN OLLAMA\n",
    "    print(f\"   ü§ñ Mengirim ke Ollama (qwen2.5:7b)...\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Anda adalah Data Engineer. Buat metadata JSON untuk tabel ini.\n",
    "    \n",
    "    NAMA TABEL: {base_name}\n",
    "    KOLOM: {', '.join(columns)}\n",
    "    CONTOH DATA: {json.dumps(context_row)}\n",
    "\n",
    "    INSTRUKSI:\n",
    "    1. Buat 'description' (deskripsi tabel).\n",
    "    2. Buat 'columns' (key: nama kolom, value: penjelasan kolom dalam Bahasa Indonesia).\n",
    "    3. Output STRICTLY JSON valid.\n",
    "    \n",
    "    FORMAT JSON:\n",
    "    {{\n",
    "      \"table\": \"{base_name}\",\n",
    "      \"description\": \"...\",\n",
    "      \"columns\": {{ \"kolom1\": \"penjelasan...\", ... }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='qwen2.5:7b',\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            format='json',\n",
    "            options={'temperature': 0.2}\n",
    "        )\n",
    "        ai_output = json.loads(response['message']['content'])\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error Ollama pada file {filename}: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. GABUNGKAN HASIL\n",
    "    final_metadata = {\n",
    "        \"table\": ai_output.get(\"table\", base_name),\n",
    "        \"description\": ai_output.get(\"description\", \"Deskripsi otomatis.\"),\n",
    "        \"columns\": ai_output.get(\"columns\", {}),\n",
    "        \"example_rows\": sample_rows\n",
    "    }\n",
    "\n",
    "    # 4. SIMPAN KE FILE JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"   ‚úÖ Disimpan: {output_path}\")\n",
    "\n",
    "\n",
    "def generate_metadata_from_folder(input_folder_path, output_folder_path):\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk iterasi folder.\n",
    "    \"\"\"\n",
    "    # Pastikan folder output ada, jika tidak buat baru\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "        print(f\"üìÅ Membuat folder output: {output_folder_path}\")\n",
    "\n",
    "    # Cari semua file CSV\n",
    "    files = [f for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not files:\n",
    "        print(\"‚ö†Ô∏è Tidak ada file CSV ditemukan di folder input.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Menemukan {len(files)} file CSV. Memulai proses...\\n\" + \"=\"*40)\n",
    "\n",
    "    # Loop setiap file\n",
    "    for file in files:\n",
    "        full_path = os.path.join(input_folder_path, file)\n",
    "        process_single_file(full_path, output_folder_path)\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\nüéâ Selesai memproses semua file.\")\n",
    "\n",
    "# --- KONFIGURASI DAN EKSEKUSI ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Tentukan folder input (tempat CSV berada)\n",
    "    INPUT_FOLDER = './data/SEKI-v2'  \n",
    "    \n",
    "    # Tentukan folder output (tempat JSON akan disimpan)\n",
    "    OUTPUT_FOLDER = './data/metadata-SEKI-v2' \n",
    "\n",
    "    generate_metadata_from_folder(INPUT_FOLDER, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3acea721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Imported ref_mkt_seki_exchange.csv ‚Üí table 'ref_mkt_seki_exchange'\n",
      "[OK] Imported ref_mkt_seki_interest.csv ‚Üí table 'ref_mkt_seki_interest'\n",
      "[OK] Imported ref_mkt_bps_jumlah_penduduk_by_usia.csv ‚Üí table 'ref_mkt_bps_jumlah_penduduk_by_usia'\n",
      "[OK] Imported ref_mkt_bps_jumlah_tenaga_kesehatan.csv ‚Üí table 'ref_mkt_bps_jumlah_tenaga_kesehatan'\n",
      "[OK] Imported ref_mkt_bps_produk_domestik_reg_bruto.csv ‚Üí table 'ref_mkt_bps_produk_domestik_reg_bruto'\n",
      "[OK] Imported ref_mkt_bps_jumlah_penduduk.csv ‚Üí table 'ref_mkt_bps_jumlah_penduduk'\n",
      "[OK] Imported ref_mkt_seki_savings.csv ‚Üí table 'ref_mkt_seki_savings'\n",
      "[OK] Imported ref_mkt_seki_ihk.csv ‚Üí table 'ref_mkt_seki_ihk'\n",
      "[OK] Imported ref_mkt_seki_pareto_terpisah.csv ‚Üí table 'ref_mkt_seki_pareto_terpisah'\n",
      "[OK] Imported ref_mkt_bps_jumlah_ibuhamil.csv ‚Üí table 'ref_mkt_bps_jumlah_ibuhamil'\n",
      "[OK] Imported ref_mkt_bps_persentase_bayi_asi_eksklusif.csv ‚Üí table 'ref_mkt_bps_persentase_bayi_asi_eksklusif'\n",
      "[OK] Imported ref_mkt_bps_umr.csv ‚Üí table 'ref_mkt_bps_umr'\n",
      "[OK] Imported ref_mkt_seki_indeks_harga.csv ‚Üí table 'ref_mkt_seki_indeks_harga'\n",
      "[OK] Imported ref_mkt_seki_investasi.csv ‚Üí table 'ref_mkt_seki_investasi'\n",
      "[OK] Imported ref_mkt_bps_gini_ratio.csv ‚Üí table 'ref_mkt_bps_gini_ratio'\n",
      "[OK] Imported ref_mkt_seki_transaksi_berjalan_internasional.csv ‚Üí table 'ref_mkt_seki_transaksi_berjalan_internasional'\n",
      "[OK] Imported ref_mkt_seki_indonesia_ringkasan.csv ‚Üí table 'ref_mkt_seki_indonesia_ringkasan'\n",
      "[OK] Imported ref_mkt_bps_jumlah_balita.csv ‚Üí table 'ref_mkt_bps_jumlah_balita'\n",
      "[OK] Imported ref_mkt_seki_devisa.csv ‚Üí table 'ref_mkt_seki_devisa'\n",
      "[OK] Imported ref_mkt_bps_angka_kelahiran.csv ‚Üí table 'ref_mkt_bps_angka_kelahiran'\n",
      "[OK] Imported ref_mkt_bps_inflasi_nasional.csv ‚Üí table 'ref_mkt_bps_inflasi_nasional'\n",
      "[OK] Imported ref_mkt_bps_jumlah_pns.csv ‚Üí table 'ref_mkt_bps_jumlah_pns'\n",
      "[OK] Imported ref_mkt_seki_inflasi.csv ‚Üí table 'ref_mkt_seki_inflasi'\n",
      "[OK] Imported ref_mkt_seki_export_import.csv ‚Üí table 'ref_mkt_seki_export_import'\n",
      "[OK] Imported ref_mkt_bps_pengeluaran_per_kapita.csv ‚Üí table 'ref_mkt_bps_pengeluaran_per_kapita'\n",
      "[OK] Imported ref_mkt_seki_pdb.csv ‚Üí table 'ref_mkt_seki_pdb'\n",
      "\n",
      "Selesai! Semua CSV sudah dimigrasikan ke database.db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "INPUT_DIR = \"data\"\n",
    "DB_PATH = \"database.db\"\n",
    "\n",
    "# Buat database\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Loop semua CSV\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if file.endswith(\".csv\"):\n",
    "        path = os.path.join(INPUT_DIR, file)\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # Nama tabel = nama file tanpa .csv\n",
    "        table_name = file.replace(\".csv\", \"\")\n",
    "\n",
    "        # Migrasi ke SQLite\n",
    "        df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "        print(f\"[OK] Imported {file} ‚Üí table '{table_name}'\")\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nSelesai! Semua CSV sudah dimigrasikan ke database.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58be6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in DB:\n",
      "- ref_mkt_seki_exchange\n",
      "- ref_mkt_seki_interest\n",
      "- ref_mkt_bps_jumlah_penduduk_by_usia\n",
      "- ref_mkt_bps_jumlah_tenaga_kesehatan\n",
      "- ref_mkt_bps_produk_domestik_reg_bruto\n",
      "- ref_mkt_bps_jumlah_penduduk\n",
      "- ref_mkt_seki_savings\n",
      "- ref_mkt_seki_ihk\n",
      "- ref_mkt_seki_pareto_terpisah\n",
      "- ref_mkt_bps_jumlah_ibuhamil\n",
      "- ref_mkt_bps_persentase_bayi_asi_eksklusif\n",
      "- ref_mkt_bps_umr\n",
      "- ref_mkt_seki_indeks_harga\n",
      "- ref_mkt_seki_investasi\n",
      "- ref_mkt_bps_gini_ratio\n",
      "- ref_mkt_seki_transaksi_berjalan_internasional\n",
      "- ref_mkt_seki_indonesia_ringkasan\n",
      "- ref_mkt_bps_jumlah_balita\n",
      "- ref_mkt_seki_devisa\n",
      "- ref_mkt_bps_angka_kelahiran\n",
      "- ref_mkt_bps_inflasi_nasional\n",
      "- ref_mkt_bps_jumlah_pns\n",
      "- ref_mkt_seki_inflasi\n",
      "- ref_mkt_seki_export_import\n",
      "- ref_mkt_bps_pengeluaran_per_kapita\n",
      "- ref_mkt_seki_pdb\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"database.db\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cur.fetchall()\n",
    "\n",
    "print(\"Tables in DB:\")\n",
    "for t in tables:\n",
    "    print(\"-\", t[0])\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68950924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"database.db\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "table = \"nama_tabel\"\n",
    "cur.execute(f\"PRAGMA table_info({table});\")\n",
    "print(cur.fetchall())\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc3b304",
   "metadata": {},
   "source": [
    "# BPS SEKI AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc2f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "user_llm = OllamaLLM(model=\"qwen2.5:7b-latest\", temperature=0)\n",
    "sql_llm = OllamaLLM(model=\"qwen-coder:latest\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3f8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "def load_metadata(path=\"metadata\"):\n",
    "    metas = {}\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith(\".json\"):\n",
    "            with open(os.path.join(path, f), \"r\", encoding=\"utf-8\") as fh:\n",
    "                metas[f.replace(\".json\",\"\")] = json.load(fh)\n",
    "    return metas\n",
    "\n",
    "metadata = load_metadata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b72a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "def log_event(event_type, data, file=\"logs.txt\"):\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"event\": event_type,\n",
    "        \"data\": data\n",
    "    }\n",
    "    with open(file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e08993e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "SQL_TEMPLATE = \"\"\"\n",
    "You are an expert SQLite SQL generator.\n",
    "User question: {question}\n",
    "\n",
    "Use ONLY the following table metadata:\n",
    "{metadata}\n",
    "\n",
    "User region = \"{region}\"\n",
    "User leveldata = \"{leveldata}\"\n",
    "\n",
    "Rules:\n",
    "- If table has 'access_column', ALWAYS filter by region.\n",
    "- Only return pure SQL without explanation.\n",
    "- No comments, no markdown, no natural language.\n",
    "\n",
    "SQL:\n",
    "\"\"\"\n",
    "\n",
    "sql_prompt = ChatPromptTemplate.from_template(SQL_TEMPLATE)\n",
    "sql_chain = sql_prompt | sql_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9729d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "forbidden = [\"drop\", \"delete\", \"update\", \"insert\", \"alter\"]\n",
    "\n",
    "def is_safe_sql(sql):\n",
    "    low = sql.lower()\n",
    "    return low.startswith(\"select\") and all(x not in low for x in forbidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5d157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_region(sql, access_column, region):\n",
    "    low = sql.lower()\n",
    "    if access_column.lower() not in low:\n",
    "        return sql  # no region needed\n",
    "\n",
    "    if f\"{access_column.lower()} =\" in low or \"where\" in low:\n",
    "        return sql\n",
    "\n",
    "    return sql + f\" WHERE {access_column} LIKE '{region}%'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7884cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def execute_sql(sql, db_path=\"database.db\"):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(sql, conn)\n",
    "    conn.close()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e8d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simple_forecast(df, date_col, val_col, periods=3):\n",
    "    df = df.dropna().sort_values(date_col)\n",
    "    X = np.arange(len(df)).reshape(-1,1)\n",
    "    y = df[val_col].astype(float).values\n",
    "\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    future_idx = np.arange(len(df), len(df)+periods).reshape(-1,1)\n",
    "    preds = model.predict(future_idx)\n",
    "\n",
    "    future_dates = pd.date_range(\n",
    "        start=df[date_col].iloc[-1], periods=periods+1, freq=\"M\"\n",
    "    )[1:]\n",
    "\n",
    "    return pd.DataFrame({date_col: future_dates, \"prediction\": preds})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c98d1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x713ab111fb00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class State(dict):\n",
    "    pass\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Node 1: Router\n",
    "def router_node(state):\n",
    "    q = state[\"question\"].lower()\n",
    "\n",
    "    if any(k in q for k in [\"prediksi\", \"forecast\", \"ramal\"]):\n",
    "        state[\"intent\"] = \"forecast\"\n",
    "    elif any(k in q for k in [\"berapa\", \"tampilkan\", \"list\", \"total\", \"select\"]):\n",
    "        state[\"intent\"] = \"sql\"\n",
    "    else:\n",
    "        state[\"intent\"] = \"clarify\"\n",
    "\n",
    "    log_event(\"router\", state)\n",
    "    return state\n",
    "\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.set_entry_point(\"router\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a799e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x713ab111fb00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def planner_node(state):\n",
    "    intent = state[\"intent\"]\n",
    "\n",
    "    if intent == \"forecast\":\n",
    "        state[\"next\"] = \"forecast_agent\"\n",
    "    elif intent == \"sql\":\n",
    "        state[\"next\"] = \"sql_agent\"\n",
    "    else:\n",
    "        state[\"next\"] = \"clarify_agent\"\n",
    "\n",
    "    log_event(\"planner\", state)\n",
    "    return state\n",
    "\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_edge(\"router\", \"planner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e581e52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x713ab111fb00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sql_agent_node(state):\n",
    "    table_meta = metadata  # pakai semua dulu\n",
    "    sql = sql_chain.run(\n",
    "        question=state[\"question\"],\n",
    "        metadata=json.dumps(table_meta),\n",
    "        region=state[\"region\"],\n",
    "        leveldata=state[\"leveldata\"]\n",
    "    )\n",
    "\n",
    "    log_event(\"sql_raw\", sql)\n",
    "\n",
    "    if not is_safe_sql(sql):\n",
    "        state[\"error\"] = \"unsafe sql\"\n",
    "        state[\"next\"] = END\n",
    "        return state\n",
    "\n",
    "    # region enforcement\n",
    "    for t, m in metadata.items():\n",
    "        access_col = m.get(\"access_column\")\n",
    "        if access_col:\n",
    "            sql = enforce_region(sql, access_col, state[\"region\"])\n",
    "\n",
    "    log_event(\"sql_final\", sql)\n",
    "\n",
    "    df = execute_sql(sql)\n",
    "    state[\"result\"] = df\n",
    "    state[\"next\"] = END\n",
    "    return state\n",
    "\n",
    "workflow.add_node(\"sql_agent\", sql_agent_node)\n",
    "workflow.add_edge(\"planner\", \"sql_agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "676d0752",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StateGraph.add_edge() got an unexpected keyword argument 'condition'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n\u001b[32m     14\u001b[39m workflow.add_node(\u001b[33m\"\u001b[39m\u001b[33mforecast_agent\u001b[39m\u001b[33m\"\u001b[39m, forecast_agent_node)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplanner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforecast_agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m==\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforecast_agent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: StateGraph.add_edge() got an unexpected keyword argument 'condition'"
     ]
    }
   ],
   "source": [
    "def forecast_agent_node(state):\n",
    "    # For forecasting, first fetch the timeseries\n",
    "    sql = \"SELECT year, value FROM some_table\"\n",
    "    df = execute_sql(sql)\n",
    "\n",
    "    log_event(\"forecast_sql\", sql)\n",
    "    log_event(\"forecast_data\", df.head().to_dict())\n",
    "\n",
    "    fc = simple_forecast(df, \"year\", \"value\", periods=3)\n",
    "    state[\"result\"] = fc\n",
    "    state[\"next\"] = END\n",
    "    return state\n",
    "\n",
    "workflow.add_node(\"forecast_agent\", forecast_agent_node)\n",
    "workflow.add_edge(\"planner\", \"forecast_agent\", condition=lambda s: s[\"next\"]==\"forecast_agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarify_agent_node(state):\n",
    "    reply = user_llm(f\"Pertanyaanmu kurang jelas: {state['question']}\")\n",
    "    state[\"result\"] = reply\n",
    "    state[\"next\"] = END\n",
    "    log_event(\"clarify\", state)\n",
    "    return state\n",
    "\n",
    "workflow.add_node(\"clarify_agent\", clarify_agent_node)\n",
    "workflow.add_edge(\"planner\", \"clarify_agent\", condition=lambda s: s[\"next\"]==\"clarify_agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Pertanyaan user: \")\n",
    "\n",
    "state = {\n",
    "    \"question\": question,\n",
    "    \"region\": \"RM III JABAR\",\n",
    "    \"leveldata\": \"2_KABUPATEN_JAWA_BARAT\"\n",
    "}\n",
    "\n",
    "result_state = graph.invoke(state)\n",
    "result_state[\"result\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
